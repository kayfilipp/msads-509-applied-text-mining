{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "import regex as re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, \\\n",
    " compile_infix_regex, compile_suffix_regex\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"C:/Users/fkrasovsky/OneDrive - Allvue Systems/Documents/usd/msads-509/Module-1-Scraping-APIs-and Research Questions\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    #Coercion to the set type gives us unique tokens.\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    \n",
    "    #there are many ways to calculate diversity, we can use TTR - which is the ratio of unique types to total tokens,\n",
    "    # or the type-token-ratio, which is calculated from dividing the unique tokens by the total number of tokens.\n",
    "    lexical_diversity = (num_unique_tokens / num_tokens)\n",
    "    \n",
    "    #this can be done by getting the length of each token passed to our function.\n",
    "    num_characters = sum([len(t) for t in text])\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        # we can accomplish this by creating a dataframe and turning it into a series.\n",
    "        n = pd.DataFrame(tokens,columns=['token'])\\\n",
    "            .value_counts()\\\n",
    "            .sort_values(ascending=False)\n",
    "        print('Five most common tokens:')\n",
    "        print(n.head(5))\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "Five most common tokens:\n",
      "token  \n",
      "text       3\n",
      "example    2\n",
      "here       2\n",
      "in         1\n",
      "is         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "**Q: Why is it beneficial to use assertion statements in your code?**\n",
    "\n",
    "A: Assertion statements allow us to create a sanity check to ensure the logic we've embedded into our code is correct in an easy, scalable way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3616f4",
   "metadata": {},
   "source": [
    "**Developer's Note: Many of the text files for smash mouth were parsed incorrectly, with the carriage return missing between the first word and last word of two lines, leaving us with frankenwords such as itDescribe, ballAnd, and more. we will write a regex function that separate the two words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e5da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deFrankenSteinify(word):\n",
    "    \n",
    "    reg_str = '\\B[A-Z]{1}.*'\n",
    "    franken_search = re.search(reg_str,word)\n",
    "    \n",
    "    if (franken_search is None or len(franken_search[0])==len(word)):\n",
    "        return [word]\n",
    "    else:\n",
    "        second_word = franken_search[0]\n",
    "        #get the index of the start of the match of the second word and get everything \n",
    "        #up until that point\n",
    "        first_word  = word[:franken_search.span()[0]]\n",
    "        return [first_word,second_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7095d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over our string, temporarily tokenize it, and create a new string where each word is no longer a frankenword.\n",
    "def deFrankenSteinifyText(text):\n",
    "    tokens = text.split()\n",
    "    out = []\n",
    "    for token in tokens:\n",
    "        this_token_list = deFrankenSteinify(token)\n",
    "        out = out + this_token_list\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74262cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Somebody once told me the World was gonna roll me'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deFrankenSteinifyText('Somebody once told me theWorld was gonna roll me')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad3290",
   "metadata": {},
   "source": [
    "#### **Lyrics Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "37d70801",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smash mouth</td>\n",
       "      <td>105\\n</td>\n",
       "      <td>\\n \\n \\n \\n Why the hell are we waitin' in lin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smash mouth</td>\n",
       "      <td>2000 Miles\\n</td>\n",
       "      <td>\\n \\n \\n \\n He's gone 2000 miles\\n It's very f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smash mouth</td>\n",
       "      <td>All Star\\n</td>\n",
       "      <td>\\n \\n \\n \\n Somebody once told me the world is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>smash mouth</td>\n",
       "      <td>Always Gets Her Way\\n</td>\n",
       "      <td>\\n \\n \\n \\n I know she likes her magazines\\n \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smash mouth</td>\n",
       "      <td>Beautiful Bomb\\n</td>\n",
       "      <td>\\n \\n \\n \\n Your asteroids bounce off her like...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        artist              song_name  \\\n",
       "0  smash mouth                  105\\n   \n",
       "1  smash mouth           2000 Miles\\n   \n",
       "2  smash mouth             All Star\\n   \n",
       "3  smash mouth  Always Gets Her Way\\n   \n",
       "4  smash mouth       Beautiful Bomb\\n   \n",
       "\n",
       "                                                text  \n",
       "0  \\n \\n \\n \\n Why the hell are we waitin' in lin...  \n",
       "1  \\n \\n \\n \\n He's gone 2000 miles\\n It's very f...  \n",
       "2  \\n \\n \\n \\n Somebody once told me the world is...  \n",
       "3  \\n \\n \\n \\n I know she likes her magazines\\n \\...  \n",
       "4  \\n \\n \\n \\n Your asteroids bounce off her like...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the lyrics data\n",
    "\n",
    "# point at the folder containing lyrics for all our artists\n",
    "lyrics_path = data_location+'/'+lyrics_folder\n",
    "artists = os.listdir(lyrics_path)\n",
    "\n",
    "#iterate over both artists, get a list of all the text files containing their songs, read them into separate dataframes.\n",
    "#we can create dynamic variable names for the dfs using the globals function.\n",
    "\n",
    "song_artist = []\n",
    "song_names = []\n",
    "song_lyrics = []\n",
    "\n",
    "for artist in artists:\n",
    "    \n",
    "    artist_path = lyrics_path + artist\n",
    "    songs = os.listdir(artist_path)\n",
    "\n",
    "    # get each song for our artist, read it in as a string, and append it to a list.\n",
    "    for song in songs:\n",
    "        song_path = artist_path + '/' + song\n",
    "        \n",
    "        with open(song_path) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # we also need a meaningful way to extract the song name. the rule for \n",
    "        # this dataset is that the title is separated by at least one carriage return,\n",
    "        # and we also know that it's always on the first line.\n",
    "        \n",
    "        this_song = lines[0]\n",
    "        this_lyrics = ' '.join(lines[1:])\n",
    "        \n",
    "        song_artist.append(artist)\n",
    "        song_names.append(this_song)\n",
    "        song_lyrics.append(this_lyrics)\n",
    "\n",
    "d = {\n",
    "        'artist':song_artist,\n",
    "        'song_name': song_names,\n",
    "        'text': song_lyrics\n",
    "    }\n",
    "\n",
    "#turn into a datafreame and do a sanity check\n",
    "df = pd.DataFrame(d)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e054c8",
   "metadata": {},
   "source": [
    "#### **Twitter Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "19f6e407",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm whatever SHHS'21 Not on Twitter much..</td>\n",
       "      <td>smashmouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>smashmouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 | i game occasionally | matching with @batb...</td>\n",
       "      <td>smashmouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22 He/Him Spotted Hyena British Autistic I lik...</td>\n",
       "      <td>smashmouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26 • Guy who games • BotW glitch enthusiast • ...</td>\n",
       "      <td>smashmouth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      artist\n",
       "0         I'm whatever SHHS'21 Not on Twitter much..  smashmouth\n",
       "1                                                NaN  smashmouth\n",
       "2  17 | i game occasionally | matching with @batb...  smashmouth\n",
       "3  22 He/Him Spotted Hyena British Autistic I lik...  smashmouth\n",
       "4  26 • Guy who games • BotW glitch enthusiast • ...  smashmouth"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the twitter data\n",
    "twitter_path = data_location+'/'+twitter_folder\n",
    "descriptions = []\n",
    "\n",
    "for file in os.listdir(twitter_path):\n",
    "    if (file.split('.')[1]=='tsv'):\n",
    "        \n",
    "        artist_name = file.split('_')[0]\n",
    "        follower_data = pd.read_csv(twitter_path+'/'+file,sep='\\t')\n",
    "        follower_data['artist'] = artist_name\n",
    "        follower_desc = follower_data[['description','artist']]\n",
    "        descriptions.append(follower_desc)\n",
    "        \n",
    "twitter_df = pd.DataFrame(descriptions[0].append(descriptions[1], \n",
    "                  ignore_index = True),columns=['description','artist'])\n",
    "twitter_df = twitter_df.rename(columns={\"description\": \"text\"})\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa5fba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpunctify(text):\n",
    "    out=text\n",
    "    for e in punctuation:\n",
    "        regstr = f'[\\{e}]'\n",
    "        out = re.sub(regstr,'',out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f8b89",
   "metadata": {},
   "source": [
    "We can also create a function that tells us how impure our corpus is for an initial inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c745283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "#create a function that keeps track of how impure our datasets are to see how much cleaning is needed and \n",
    "#if our cleaning had any meaningful effect.\n",
    "\n",
    "def impurity(text, min_len=10):\n",
    "    \"\"\"returns the share of suspicious characters in a text\"\"\"\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bfcca4",
   "metadata": {},
   "source": [
    "Finally, we want a function that both tokenizes and removes stop words from the corpus as well as lemmatizes it.\n",
    "**The problem with using punctuated stop words for removal is that by the time we get to stop word removal, our corpus will have been tokenized and stripped of punctuation. as a result, we need to add the ``sw`` list to our nlp object without any punctuation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f06f5ab0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'\\S+').match)\n",
    "for i in sw:\n",
    "    unpunctsw = re.sub('[^a-zA-Z]','',i)\n",
    "    #print(i,unpunctsw)\n",
    "    nlp.vocab[unpunctsw].is_stop = True\n",
    "    nlp.vocab[i].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4b0ba697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    doc = nlp(text)\n",
    "    return [t.text.strip()\n",
    "            for t in doc \n",
    "            if (not t.is_stop) \n",
    "            and (not t.is_punct)\n",
    "            and (not t.is_space) \n",
    "            #and t.pos_ in ['NOUN', 'PROPN','VERB']\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ade9bd",
   "metadata": {},
   "source": [
    "### Cleaning Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b327033a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69723</th>\n",
       "      <td>{♉} {17} {pansexual} {🎨} {she/they}</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74582</th>\n",
       "      <td>[̲̅a̲̅] [̲̅r̲̅] [̲̅t̲̅] [̲̅i̲̅] [̲̅s̲̅] [̲̅t̲̅]</td>\n",
       "      <td>0.255319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116601</th>\n",
       "      <td>&amp;:&amp;,$7/&amp;.'dhuiskxn$&amp;3&amp;n</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103596</th>\n",
       "      <td>치치🍒 #리사 #로제 #제니 #지수</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3214</th>\n",
       "      <td>¥¥{§÷÷{¶¶{÷) #=/2486DEG]¥D#=+%54¶¦|÷]¿]¥©}÷</td>\n",
       "      <td>0.209302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  impurity\n",
       "69723               {♉} {17} {pansexual} {🎨} {she/they}  0.285714\n",
       "74582   [̲̅a̲̅] [̲̅r̲̅] [̲̅t̲̅] [̲̅i̲̅] [̲̅s̲̅] [̲̅t̲̅]  0.255319\n",
       "116601                          &:&,$7/&.'dhuiskxn$&3&n  0.217391\n",
       "103596                              치치🍒 #리사 #로제 #제니 #지수  0.210526\n",
       "3214        ¥¥{§÷÷{¶¶{÷) #=/2486DEG]¥D#=+%54¶¦|÷]¿]¥©}÷  0.209302"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  make sure everything has been cast to string.\n",
    "twitter_df['text'] = twitter_df['text'].map(str)\n",
    "#  calculate the impurity of our text\n",
    "twitter_df['impurity'] = twitter_df['text'].apply(impurity,min_len=10)\n",
    "# get the top 5 records\n",
    "twitter_df[['text', 'impurity']].sort_values(by='impurity', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55f54a3",
   "metadata": {},
   "source": [
    "Initial observation suggests that twitter data is incredibly messy in edge cases. We begin by removing punctuation characters, splitting on whitespace, folding to lowercase, and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a8e10de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. remove punctuation\n",
    "twitter_df['clean_text'] = twitter_df['text'].map(unpunctify)\n",
    "\n",
    "# 2. make lowercase\n",
    "twitter_df['clean_text'] = twitter_df['clean_text'].map(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "373d359a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>artist</th>\n",
       "      <th>impurity</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm whatever SHHS'21 Not on Twitter much..</td>\n",
       "      <td>smashmouth</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>im whatever shhs21 not on twitter much</td>\n",
       "      <td>[im, shhs21, twitter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nan</td>\n",
       "      <td>smashmouth</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 | i game occasionally | matching with @batb...</td>\n",
       "      <td>smashmouth</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17  i game occasionally  matching with batben55</td>\n",
       "      <td>[17, game, occasionally, matching, batben55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22 He/Him Spotted Hyena British Autistic I lik...</td>\n",
       "      <td>smashmouth</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>22 hehim spotted hyena british autistic i like...</td>\n",
       "      <td>[22, hehim, spotted, hyena, british, autistic,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26 • Guy who games • BotW glitch enthusiast • ...</td>\n",
       "      <td>smashmouth</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>26 • guy who games • botw glitch enthusiast • ...</td>\n",
       "      <td>[26, guy, games, botw, glitch, enthusiast, ban...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      artist  impurity  \\\n",
       "0         I'm whatever SHHS'21 Not on Twitter much..  smashmouth  0.000000   \n",
       "1                                                nan  smashmouth  0.000000   \n",
       "2  17 | i game occasionally | matching with @batb...  smashmouth  0.000000   \n",
       "3  22 He/Him Spotted Hyena British Autistic I lik...  smashmouth  0.013158   \n",
       "4  26 • Guy who games • BotW glitch enthusiast • ...  smashmouth  0.025974   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0             im whatever shhs21 not on twitter much   \n",
       "1                                                nan   \n",
       "2   17  i game occasionally  matching with batben55    \n",
       "3  22 hehim spotted hyena british autistic i like...   \n",
       "4  26 • guy who games • botw glitch enthusiast • ...   \n",
       "\n",
       "                                              tokens  \n",
       "0                              [im, shhs21, twitter]  \n",
       "1                                              [nan]  \n",
       "2       [17, game, occasionally, matching, batben55]  \n",
       "3  [22, hehim, spotted, hyena, british, autistic,...  \n",
       "4  [26, guy, games, botw, glitch, enthusiast, ban...  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. remove stop words and split on whitespace by modifying the spacey NLP object. we can also filter out spaces.\n",
    "# THIS FUNCTION ALSO TOKENIZES AND LEMMATIZES.\n",
    "twitter_df['tokens'] = twitter_df['clean_text'].apply(remove_stop_words)\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63d222",
   "metadata": {},
   "source": [
    "**To see if our efforts worked, let's examine the text one more time for impurity.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "02471321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im whatever shhs21 not on twitter much</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79834</th>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79846</th>\n",
       "      <td>me is here</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79845</th>\n",
       "      <td>sheher</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79844</th>\n",
       "      <td>quackity acepto sobornos</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   clean_text  impurity\n",
       "0      im whatever shhs21 not on twitter much       0.0\n",
       "79834                                     nan       0.0\n",
       "79846                              me is here       0.0\n",
       "79845                                  sheher       0.0\n",
       "79844                quackity acepto sobornos       0.0"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['impurity'] = twitter_df['clean_text'].apply(impurity,min_len=10)\n",
    "# get the top 5 records\n",
    "twitter_df[['clean_text', 'impurity']].sort_values(by='impurity', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "df94656b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>artist</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm whatever SHHS'21 Not on Twitter much..</td>\n",
       "      <td>smashmouth</td>\n",
       "      <td>im whatever shhs21 not on twitter much</td>\n",
       "      <td>[im, shhs21, twitter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nan</td>\n",
       "      <td>smashmouth</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17 | i game occasionally | matching with @batb...</td>\n",
       "      <td>smashmouth</td>\n",
       "      <td>17  i game occasionally  matching with batben55</td>\n",
       "      <td>[17, game, occasionally, matching, batben55]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22 He/Him Spotted Hyena British Autistic I lik...</td>\n",
       "      <td>smashmouth</td>\n",
       "      <td>22 hehim spotted hyena british autistic i like...</td>\n",
       "      <td>[22, hehim, spotted, hyena, british, autistic,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26 • Guy who games • BotW glitch enthusiast • ...</td>\n",
       "      <td>smashmouth</td>\n",
       "      <td>26 • guy who games • botw glitch enthusiast • ...</td>\n",
       "      <td>[26, guy, games, botw, glitch, enthusiast, ban...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text      artist  \\\n",
       "0         I'm whatever SHHS'21 Not on Twitter much..  smashmouth   \n",
       "1                                                nan  smashmouth   \n",
       "2  17 | i game occasionally | matching with @batb...  smashmouth   \n",
       "3  22 He/Him Spotted Hyena British Autistic I lik...  smashmouth   \n",
       "4  26 • Guy who games • BotW glitch enthusiast • ...  smashmouth   \n",
       "\n",
       "                                                text  \\\n",
       "0             im whatever shhs21 not on twitter much   \n",
       "1                                                nan   \n",
       "2   17  i game occasionally  matching with batben55    \n",
       "3  22 hehim spotted hyena british autistic i like...   \n",
       "4  26 • guy who games • botw glitch enthusiast • ...   \n",
       "\n",
       "                                              tokens  \n",
       "0                              [im, shhs21, twitter]  \n",
       "1                                              [nan]  \n",
       "2       [17, game, occasionally, matching, batben55]  \n",
       "3  [22, hehim, spotted, hyena, british, autistic,...  \n",
       "4  [26, guy, games, botw, glitch, enthusiast, ban...  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df = twitter_df.rename(columns={\"clean_text\": \"text\", \"text\": \"raw_text\"})\n",
    "twitter_df = twitter_df.drop(columns=['impurity'])\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596652dd",
   "metadata": {},
   "source": [
    "### Cleaning Lyrics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08751874",
   "metadata": {},
   "source": [
    "**Developer's Note: The unique circumstances of our lyrics data require us to un-frankenstein lines that were mashed together by running the corpus through one additional element in the pipeline, the frankensteinify function we made earlier, after we tokenize words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e0f22e10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>\\n \\n \\n [orig. performed by WAR]\\n \\n \\n Why ...</td>\n",
       "      <td>0.013917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n \\n \\n \\n You must admit that\\n You look lik...</td>\n",
       "      <td>0.009891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>\\n \\n \\n \\n Paintings of dogs playing pool\\n \\...</td>\n",
       "      <td>0.008639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>\\n \\n \\n [Verse:]\\n I canÃ¢Â€Â™t help look in ...</td>\n",
       "      <td>0.007801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\\n \\n \\n \\n Today's escape will consist of a m...</td>\n",
       "      <td>0.007788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  impurity\n",
       "92  \\n \\n \\n [orig. performed by WAR]\\n \\n \\n Why ...  0.013917\n",
       "9   \\n \\n \\n \\n You must admit that\\n You look lik...  0.009891\n",
       "56  \\n \\n \\n \\n Paintings of dogs playing pool\\n \\...  0.008639\n",
       "68  \\n \\n \\n [Verse:]\\n I canÃ¢Â€Â™t help look in ...  0.007801\n",
       "29  \\n \\n \\n \\n Today's escape will consist of a m...  0.007788"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your clean lyrics data here\n",
    "\n",
    "#  make sure everything has been cast to string.\n",
    "df['text'] = df['text'].map(str)\n",
    "\n",
    "# calculate the impurity of our text\n",
    "df['impurity'] = df['text'].apply(impurity,min_len=10)\n",
    "# get the first 5 more impure lyrics\n",
    "df[['text', 'impurity']].sort_values(by='impurity', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736dd57",
   "metadata": {},
   "source": [
    "we're not particularly worried about impurity in this case - our worst candidate is less than 1.5% impure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a60321d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. remove punctuation\n",
    "df['text'] = df['text'].map(unpunctify)\n",
    "# 1b. remove newlines\n",
    "df['text'] = df['text'].apply(lambda x: x.replace(\"\\n\", \"\"))\n",
    "# 2. unfrankensteinify!\n",
    "df['text'] = df['text'].map(deFrankenSteinifyText)\n",
    "# 3. lowercase\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "# 4. tokenize and get rid of stop words and lemmatize\n",
    "df['tokens'] = df['text'].apply(remove_stop_words)\n",
    "# 5. morbid curiosity\n",
    "df['num_tokens'] = df['tokens'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bb114dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_name</th>\n",
       "      <th>text</th>\n",
       "      <th>impurity</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smash mouth</td>\n",
       "      <td>105\\n</td>\n",
       "      <td>why the hell are we waitin in line a billion c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[hell, waitin, line, billion, cars, going, way...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smash mouth</td>\n",
       "      <td>2000 Miles\\n</td>\n",
       "      <td>hes gone 2000 miles its very far the snow is f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[hes, gone, 2000, miles, far, snow, falling, g...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smash mouth</td>\n",
       "      <td>All Star\\n</td>\n",
       "      <td>somebody once told me the world is gonna roll ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[somebody, told, world, gonna, roll, aint, sha...</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>smash mouth</td>\n",
       "      <td>Always Gets Her Way\\n</td>\n",
       "      <td>i know she likes her magazines of what theyre ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[know, likes, magazines, theyre, wearing, holl...</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smash mouth</td>\n",
       "      <td>Beautiful Bomb\\n</td>\n",
       "      <td>your asteroids bounce off her like a trampolin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[asteroids, bounce, like, trampoline, shake, l...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        artist              song_name  \\\n",
       "0  smash mouth                  105\\n   \n",
       "1  smash mouth           2000 Miles\\n   \n",
       "2  smash mouth             All Star\\n   \n",
       "3  smash mouth  Always Gets Her Way\\n   \n",
       "4  smash mouth       Beautiful Bomb\\n   \n",
       "\n",
       "                                                text  impurity  \\\n",
       "0  why the hell are we waitin in line a billion c...       0.0   \n",
       "1  hes gone 2000 miles its very far the snow is f...       0.0   \n",
       "2  somebody once told me the world is gonna roll ...       0.0   \n",
       "3  i know she likes her magazines of what theyre ...       0.0   \n",
       "4  your asteroids bounce off her like a trampolin...       0.0   \n",
       "\n",
       "                                              tokens  num_tokens  \n",
       "0  [hell, waitin, line, billion, cars, going, way...          77  \n",
       "1  [hes, gone, 2000, miles, far, snow, falling, g...          58  \n",
       "2  [somebody, told, world, gonna, roll, aint, sha...         182  \n",
       "3  [know, likes, magazines, theyre, wearing, holl...          75  \n",
       "4  [asteroids, bounce, like, trampoline, shake, l...          48  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c97ab",
   "metadata": {},
   "source": [
    "**Before we do this, we need a function that can ingest and combine every token.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9dffc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tokens(tokens):\n",
    "    out = []\n",
    "    for token_list in tokens:\n",
    "        out = out + token_list\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate our lyrics\n",
    "smash_mouth = df.query(\"artist=='smash mouth'\")\n",
    "wallows = df.query(\"artist!='smash mouth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "22dcf0b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4613 tokens in the data.\n",
      "There are 997 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.216 in the data.\n",
      "Five most common tokens:\n",
      "token\n",
      "im       139\n",
      "know     131\n",
      "ill      104\n",
      "like      83\n",
      "need      77\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4613, 997, 0.21612833297203554, 55]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on wallows lyrics\n",
    "wallows_lyrics_tokens = combine_tokens(wallows['tokens'])\n",
    "descriptive_stats(wallows_lyrics_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5f408db8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9899 tokens in the data.\n",
      "There are 2413 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.244 in the data.\n",
      "Five most common tokens:\n",
      "token    \n",
      "im           187\n",
      "oh           132\n",
      "know         127\n",
      "christmas    122\n",
      "got          122\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9899, 2413, 0.2437619961612284, 55]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on smash mouth lyrics\n",
    "smash_mouth_lyrics_tokens = combine_tokens(smash_mouth['tokens'])\n",
    "descriptive_stats(smash_mouth_lyrics_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "797602e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate our twitter data\n",
    "smash_mouth = twitter_df.query(\"artist=='smashmouth'\")\n",
    "wallows = twitter_df.query(\"artist!='smashmouth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "50f88995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on wallows twitter data\n",
    "wallows_twitter_data = combine_tokens(wallows['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e445cc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 213127 tokens in the data.\n",
      "There are 56410 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.265 in the data.\n",
      "Five most common tokens:\n",
      "token \n",
      "nan       16359\n",
      "sheher     3034\n",
      "͏          1601\n",
      "love       1422\n",
      "im         1116\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[213127, 56410, 0.2646778681255777, 55]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stats(wallows_twitter_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "99d9941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on smash mouth twitter data\n",
    "smashmouth_twitter_data = combine_tokens(smash_mouth['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0e7598c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 345338 tokens in the data.\n",
      "There are 76162 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.221 in the data.\n",
      "Five most common tokens:\n",
      "token \n",
      "nan       11879\n",
      "sheher     3247\n",
      "hehim      2753\n",
      "im         2397\n",
      "like       1761\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[345338, 76162, 0.2205433517307681, 55]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stats(smashmouth_twitter_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: There would probably be more filler words and articles such as \"the\",\"and\",\"or\", etc.\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: I am not surprised in the slightest that smash mouth has a low lexical diversity, but I am shocked that they have a higher score than the wallows - I think this might be on account of the sheer number of songs they've written.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_emoji(s):\n",
    "    return(s in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "assert(is_emoji(\"❤️\"))\n",
    "assert(not is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "## Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1ac91334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_emojis(desc):\n",
    "    args = desc.split()\n",
    "    return [arg for arg in args if is_emoji(arg)]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "269cd433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fkrasovsky\\AppData\\Local\\Temp\\ipykernel_19648\\2231112070.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wallows['emojis'] = wallows['raw_text'].map(find_emojis)\n",
      "C:\\Users\\fkrasovsky\\AppData\\Local\\Temp\\ipykernel_19648\\2231112070.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  smash_mouth['emojis'] = smash_mouth['raw_text'].map(find_emojis)\n"
     ]
    }
   ],
   "source": [
    "#iterate over all the raw text and move emojis into a new column\n",
    "wallows['emojis'] = wallows['raw_text'].map(find_emojis)\n",
    "smash_mouth['emojis'] = smash_mouth['raw_text'].map(find_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c337571",
   "metadata": {},
   "source": [
    "### Ten Most Common Emojis for Smash Mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "53be72bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🏳️\\u200d🌈', 478), ('✨', 280), ('❤️', 243), ('🏳️\\u200d⚧️', 163), ('🔞', 161), ('💜', 138), ('🌈', 121), ('🖤', 116), ('❤', 110), ('💙', 95)]\n"
     ]
    }
   ],
   "source": [
    "emojis_sm = combine_tokens(smash_mouth['emojis'])\n",
    "print(Counter(emojis_sm).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198a6e2",
   "metadata": {},
   "source": [
    "### Ten Most Common Emojis for Wallows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4072ba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🏳️\\u200d🌈', 517), ('✨', 296), ('❤️', 164), ('🤍', 136), ('🖤', 110), ('🇲🇽', 86), ('🦋', 81), ('🌈', 78), ('💜', 78), ('💙', 74)]\n"
     ]
    }
   ],
   "source": [
    "emojis_ws = combine_tokens(wallows['emojis'])\n",
    "print(Counter(emojis_ws).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "## Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the regex for a hashtag is defined as one (#) character followed by an ubroken chain of letters and numbers.\n",
    "# we also want to coerce to lowercase for more insight.\n",
    "def find_hashtags(desc):\n",
    "    regStr ='#{1}[a-zA-z0-9]+'\n",
    "    hashtags = re.findall(regStr,desc)\n",
    "    return list(map(lambda x: x.lower(),hashtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "3531a2c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fkrasovsky\\AppData\\Local\\Temp\\ipykernel_19648\\3540275004.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wallows['hashtags'] = wallows['raw_text'].map(find_hashtags)\n",
      "C:\\Users\\fkrasovsky\\AppData\\Local\\Temp\\ipykernel_19648\\3540275004.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  smash_mouth['hashtags'] = smash_mouth['raw_text'].map(find_hashtags)\n"
     ]
    }
   ],
   "source": [
    "#iterate over all the raw text and move hashtags into a new column\n",
    "wallows['hashtags'] = wallows['raw_text'].map(find_hashtags)\n",
    "smash_mouth['hashtags'] = smash_mouth['raw_text'].map(find_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc46fda",
   "metadata": {},
   "source": [
    "### Ten Most Common Hashtags for Smash Mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "a991d6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#blacklivesmatter', 407), ('#blm', 302), ('#1', 138), ('#resist', 100), ('#actuallyautistic', 50), ('#stopasianhate', 41), ('#acab', 40), ('#theresistance', 34), ('#freepalestine', 32), ('#resistance', 31)]\n"
     ]
    }
   ],
   "source": [
    "emojis_sm = combine_tokens(smash_mouth['hashtags'])\n",
    "print(Counter(emojis_sm).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39b4cc",
   "metadata": {},
   "source": [
    "### Ten Most Common Hashtags for Wallows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "f57983c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#1', 183), ('#blacklivesmatter', 71), ('#harry', 50), ('#blm', 43), ('#freepalestine', 28), ('#wallows', 27), ('#louis', 27), ('#stopasianhate', 23), ('#bts', 21), ('#marvel', 15)]\n"
     ]
    }
   ],
   "source": [
    "emojis_ws = combine_tokens(wallows['hashtags'])\n",
    "print(Counter(emojis_ws).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "56ae38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def song_name_tokenize(song_name):\n",
    "    doc = nlp(song_name)\n",
    "    return [t for t in doc if not t.is_punct and not t.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8f4ceef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean up our song names before we tokenize them.\n",
    "df['song_name_clean'] = df['song_name'].apply(lambda x: x.replace(\"\\n\", \"\"))\n",
    "df['song_name_tokens'] = df['song_name_clean'].map(song_name_tokenize)\n",
    "\n",
    "# separate our lyrics df again\n",
    "smash_mouth = df.query(\"artist=='smash mouth'\")\n",
    "wallows = df.query(\"artist!='smash mouth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daadaf0a",
   "metadata": {},
   "source": [
    "### Five Most Common Words in Song Titles for Smash Mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "92bd3dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(105, 1), (2000, 1), (Miles, 1), (All, 1), (Star, 1)]\n"
     ]
    }
   ],
   "source": [
    "titles_sm = combine_tokens(smash_mouth['song_name_tokens'])\n",
    "print(Counter(titles_sm).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede3220",
   "metadata": {},
   "source": [
    "### Five Most Common Words in Song Titles for Wallows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f3f8a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1980s, 1), (Horror, 1), (Film, 1), (Another, 1), (Story, 1)]\n"
     ]
    }
   ],
   "source": [
    "titles_ws = combine_tokens(wallows['song_name_tokens'])\n",
    "print(Counter(titles_ws).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Artist 1    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Artist 2    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "rdf = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "rdf.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: one or more whitespaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'[\\s]+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "2294c440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "smash mouth    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "wallows        AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: new_num_tokens, dtype: object"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your lyric length comparison chart here. \n",
    "df['new_tokenized_lyrics'] = df['text'].map(tokenize_lyrics)\n",
    "df['new_num_tokens'] = df['new_tokenized_lyrics'].map(len)\n",
    "df.groupby('artist')['new_num_tokens'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
