# MSADS-509-applied-text-mining

## Module 1: Data Acquisition with APIs and Scraping
<i>Template URL: https://github.com/37chandler/ads-tm-api-scrape </i>
<p>Technologies Used: Python, Jupyter, Tweepy</p>

After finding two musical artists with 100k+ followers on twitter and at least 20 songs,
<ul>
  <li><b>Pull descriptions from twitter about each of those artists' followers</b></li>
  <li>Pull and store the lyrics for both artists.</li>
</ul>

### Breakdown of Scraping Process
<img src='https://user-images.githubusercontent.com/36943200/167715336-b200af78-e173-4e2b-acde-2212f99f4126.png'></img>

## Module 2:Tokenization, Normalization, Descriptive Statistics
<i>Template URL: [ ](https://github.com/37chandler/ads-tm-token-norm/)</i>
<p>Technologies Used: Python, Jupyter</p>

## Module 3: Tokenization, Normalization, Descriptive Statistics
<i>Template URL: https://github.com/37chandler/ads-tm-group-comp</i>
<p>Technologies Used: Python, Jupyter</p>

## Module 4: Naive Bayes in text mining
<i>Template URL: https://github.com/37chandler/tm-nb-conventions </i>
<p>Technologies Used: Python, Jupyter</p>

## Module 5: Unsupervised Learning Methods
<i>Template URL: https://github.com/37chandler/ads-tm-topic-modeling </i>
<p>Technologies Used: Python, Jupyter</p>
<p>
  <ul>
    <li>LDA - Latent Dirichlet Allocation: A stochastic method for discovering document topics with a lot of flexibility.</li>
    <li>Non negative matrix factorization</li>
    <li>singular-value decomposition (LSI)</li>
  </ul>
</p>

## Module 6: Sentiment Analysis
Lexicon based approaches such as Bing-Liu, VADER, etc. for classifying the sentiment of a piece of text based on a score generated from the amount <br>
of words belonging to a positive and a negative lexicon, as well as the use of **linearSVC** and BERT - a pretrained model that is trained on the combined English Wikipedia and books corpus using a masked-language-model (MLM). MLM refers to the practice of masking one word at a time in a document and engaging in transfer learning by evaluating how well the model can predict what the missing word is.

